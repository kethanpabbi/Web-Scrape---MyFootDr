{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fd148bf",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca933e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 existing clinics\n",
      "Regions found: 11\n",
      "REGION brisbane: 31 clinics\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/albany-creek-allsports-podiatry/\n",
      "    ADDED: Allsports Podiatry Albany Creek\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/aspley-allsports-podiatry/\n",
      "    ADDED: My FootDr Allsports Podiatry Aspley\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/brisbane-cbd-podiatry-centre/\n",
      "    ADDED: My FootDr Brisbane CBD\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/brookwater-podiatry-centre/\n",
      "    ADDED: My FootDr Brookwater\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/calamvale-podiatry-centre-allsports/\n",
      "    ADDED: Allsports Podiatry Calamvale\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/camp-hill-podiatry-centre-allsports/\n",
      "    ADDED: Allsports Podiatry Camp Hill\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/camp-hill-podiatry-centre/\n",
      "    ADDED: My FootDr Camp Hill\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/cleveland-podiatry-centre/\n",
      "    ADDED: My FootDr Cleveland\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/forest-lake-podiatry-centre-allsports/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/fortitude-valley-podiatry-centre/\n",
      "    ADDED: My FootDr Fortitude Valley\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/gumdale-podiatry-centre/\n",
      "    ADDED: My FootDr Gumdale\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/hawthorne-podiatry-clinic-allsports-podiatry/\n",
      "    ADDED: My FootDr Allsports Podiatry Hawthorne\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/indooroopilly-podiatry-centre-allsports/\n",
      "    ADDED: Allsports Podiatry Indooroopilly\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/indooroopilly-podiatry-centre/\n",
      "    ADDED: My FootDr Indooroopilly\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/ipswich-podiatry-centre/\n",
      "    ADDED: My FootDr Ipswich\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/jindalee-podiatry-centre-allsports/\n",
      "    ADDED: My FootDr Jindalee\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/kangaroo-point-allsports-podiatry-centre/\n",
      "    ADDED: Allsports Podiatry Kangaroo Point\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/mitchelton-podiatry-centre/\n",
      "    ADDED: My FootDr Brookside\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/mountgravatt-podiatry-orthotics-customfootwear/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/newstead-podiatry-clinic-anytime-physio-podiatry/\n",
      "    ADDED: Anytime Physio & Podiatry Newstead\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/north-lakes-podiatry-centre/\n",
      "    ADDED: My FootDr North Lakes\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/red-hill-podiatry-centre-allsports/\n",
      "    ADDED: Allsports Podiatry Red Hill\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/red-hill/\n",
      "    ADDED: My FootDr Red Hill\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/redcliffe-podiatry-centre/\n",
      "    ADDED: My FootDr Redcliffe\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/shailer-park-podiatry-centre/\n",
      "    ADDED: My FootDr Shailer Park\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/stafford-podiatry-centre/\n",
      "    ADDED: My FootDr Stafford\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/the-gap-podiatry-centre-allsports/\n",
      "    ADDED: Allsports Podiatry The Gap\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/toowong-podiatry-centre-allsports/\n",
      "    ADDED: Allsports Podiatry Toowong\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/toowoomba-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/wavell-heights-podiatry-centre-allsports/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618202752/https://www.myfootdr.com.au/our-clinics/wellington-point-podiatry-centre-allsports/\n",
      "    FAILED\n",
      "REGION central-queensland: 8 clinics\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/bargara-advanced-foot-care-podiatry-clinic/\n",
      "    ADDED: Advanced Foot Care Bargara\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/bundaberg-advanced-foot-care-podiatry-centre/\n",
      "    ADDED: Advanced Foot Care Bundaberg\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/gladstone-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/hervey-bay-advanced-foot-care-podiatry-centre/\n",
      "    ADDED: Advanced Foot Care Hervey Bay\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/mackay-podiatry-centre/\n",
      "    ADDED: My FootDr Mackay\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/monto-advanced-foot-care-podiatry-clinic/\n",
      "    ADDED: Advanced Foot Care Monto\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/rockhampton-podiatry-centre/\n",
      "    ADDED: My FootDr Rockhampton\n",
      "  Scraping http://web.archive.org/web/20250707223226/https://www.myfootdr.com.au/our-clinics/yeppoon-podiatry-centre/\n",
      "    ADDED: My FootDr Yeppoon\n",
      "REGION gold-coast: 7 clinics\n",
      "  Scraping http://web.archive.org/web/20250829222349/https://www.myfootdr.com.au/our-clinics/allsports-podiatry-parkwood/\n",
      "    ADDED: My FootDr Allsports Podiatry Parkwood\n",
      "  Scraping http://web.archive.org/web/20250829222349/https://www.myfootdr.com.au/our-clinics/bundall-back-in-motion-podiatry/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250829222349/https://www.myfootdr.com.au/our-clinics/burleigh-waters-back-in-motion-podiatry/\n",
      "    ADDED: Back In Motion Podiatry Burleigh Waters\n",
      "  Scraping http://web.archive.org/web/20250829222349/https://www.myfootdr.com.au/our-clinics/hope-island-podiatry-centre/\n",
      "    ADDED: My FootDr Hope Island\n",
      "  Scraping http://web.archive.org/web/20250829222349/https://www.myfootdr.com.au/our-clinics/pimpama-allsports-podiatry/\n",
      "    ADDED: Allsports Podiatry Pimpama\n",
      "  Scraping http://web.archive.org/web/20250829222349/https://www.myfootdr.com.au/our-clinics/robina-physiologic-podiatry-centre/\n",
      "    ADDED: Physiologic Podiatry Robina\n",
      "  Scraping http://web.archive.org/web/20250829222349/https://www.myfootdr.com.au/our-clinics/robina-podiatry-centre/\n",
      "    FAILED\n",
      "REGION FAILED: new-south-wales\n",
      "REGION north-queensland: 5 clinics\n",
      "  Scraping http://web.archive.org/web/20250618214139/https://www.myfootdr.com.au/our-clinics/cairns-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618214139/https://www.myfootdr.com.au/our-clinics/cowboys-centre-of-excellence-nq-foot-ankle-centre/\n",
      "    ADDED: NQ Foot & Ankle Centre Cowboys Centre of Excellence\n",
      "  Scraping http://web.archive.org/web/20250618214139/https://www.myfootdr.com.au/our-clinics/my-footdr-nq-foot/\n",
      "    ADDED: NQ Foot & Ankle Centre Kirwan\n",
      "  Scraping http://web.archive.org/web/20250618214139/https://www.myfootdr.com.au/our-clinics/thuringowa-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618214139/https://www.myfootdr.com.au/our-clinics/townsville-podiatry-centre/\n",
      "    ADDED: My FootDr Townsville\n",
      "REGION FAILED: northern-territory\n",
      "REGION FAILED: south-australia\n",
      "REGION sunshine-coast: 2 clinics\n",
      "  Scraping http://web.archive.org/web/20250618212909/https://www.myfootdr.com.au/our-clinics/beerwah-podiatry-clinic/\n",
      "    ADDED: My FootDr Beerwah\n",
      "  Scraping http://web.archive.org/web/20250618212909/https://www.myfootdr.com.au/our-clinics/noosa/\n",
      "    ADDED: Allsports Podiatry Noosa\n",
      "REGION tasmania: 2 clinics\n",
      "  Scraping http://web.archive.org/web/20250516141742/https://www.myfootdr.com.au/our-clinics/devonport-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250516141742/https://www.myfootdr.com.au/our-clinics/south-hobart-ispahan-podiatry-centre/\n",
      "    ADDED: My FootDr Ispahan\n",
      "REGION victoria: 15 clinics\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/back-in-motion-podiatry-bacchus-marsh/\n",
      "    ADDED: Back In Motion Podiatry Bacchus Marsh\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/back-in-motion-podiatry-melton/\n",
      "    ADDED: Back In Motion Podiatry Melton\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/ballarat-podiatry-centre/\n",
      "    ADDED: My FootDr Ballarat\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/bayswater-back-in-motion-podiatry/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/boronia-podiatry-centre/\n",
      "    ADDED: My FootDr Boronia\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/chadstone-the-foot-and-ankle-clinic-podiatry-clinic/\n",
      "    ADDED: The Foot and Ankle Clinic Podiatry Chadstone\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/cranbourne-podiatry-centre/\n",
      "    ADDED: My FootDr Cranbourne\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/east-bentleigh-the-foot-and-ankle-clinic-podiatry-clinic/\n",
      "    ADDED: The Foot and Ankle Clinic Podiatry East Bentleigh\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/moe-the-foot-and-ankle-clinic-podiatry-clinic/\n",
      "    ADDED: The Foot and Ankle Clinic Moe\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/pakenham-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/sale-the-foot-and-ankle-clinic-podiatry-clinic/\n",
      "    ADDED: The Foot and Ankle Clinic Sale\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/traralgon-the-foot-and-ankle-clinic-podiatry-clinic/\n",
      "    ADDED: The Foot and Ankle Clinic Traralgon\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/wantirna-podiatry-centre/\n",
      "    ADDED: My FootDr Wantirna\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/warragul-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250618234903/https://www.myfootdr.com.au/our-clinics/wodonga-podiatry-centre/\n",
      "    ADDED: Border Podiatry Wodonga\n",
      "REGION western-australia: 5 clinics\n",
      "  Scraping http://web.archive.org/web/20250707232629/https://www.myfootdr.com.au/our-clinics/bim-podiatry-balcatta/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250707232629/https://www.myfootdr.com.au/our-clinics/como-back-in-motion-podiatry/\n",
      "    ADDED: Back In Motion Podiatry Como\n",
      "  Scraping http://web.archive.org/web/20250707232629/https://www.myfootdr.com.au/our-clinics/currambine-podiatry-centre/\n",
      "    FAILED\n",
      "  Scraping http://web.archive.org/web/20250707232629/https://www.myfootdr.com.au/our-clinics/warwick-podiatry-centre/\n",
      "    ADDED: My FootDr Warwick\n",
      "  Scraping http://web.archive.org/web/20250707232629/https://www.myfootdr.com.au/our-clinics/wembley-downs-podiatry-centre/\n",
      "    ADDED: My FootDr Wembley Downs\n",
      "Saved 59 clinics\n",
      "Failed regions: 3\n",
      "Failed clinics: 16\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "CSV_FILE = \"./myfootdr_clinics.csv\"\n",
    "FAILED_REGIONS_FILE = \"./failed_regions.txt\"\n",
    "FAILED_CLINICS_FILE = \"./failed_clinics.txt\"\n",
    "\n",
    "DEFAULT_SNAPSHOTS = [\n",
    "    \"20250708180027\",\n",
    "    \"20250517063937\",\n",
    "    \"20250516141742\",\n",
    "]\n",
    "\n",
    "REGION_SNAPSHOTS = {\n",
    "    \"victoria\": [\"20250618234903\"],\n",
    "    \"western-australia\": [\"20250707232629\"],\n",
    "}\n",
    "\n",
    "BASE_SITE = \"https://www.myfootdr.com.au\"\n",
    "ARCHIVE_BASE = \"http://web.archive.org/web\"\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"Name of Clinic\",\n",
    "    \"Address\",\n",
    "    \"Email\",\n",
    "    \"Phone\",\n",
    "    \"Services\",\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept-Encoding\": \"identity\",\n",
    "    \"Connection\": \"close\",\n",
    "})\n",
    "\n",
    "def build_url(snapshot, path):\n",
    "    return f\"{ARCHIVE_BASE}/{snapshot}/{BASE_SITE}{path}\"\n",
    "\n",
    "def get_soup(url, retries=3):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            time.sleep(1.2)\n",
    "            r = session.get(url, timeout=40)\n",
    "            if r.status_code == 200 and len(r.text) > 500:\n",
    "                return BeautifulSoup(r.text, \"html.parser\")\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def get_soup_with_snapshots(path, snapshots):\n",
    "    for snap in snapshots:\n",
    "        soup = get_soup(build_url(snap, path))\n",
    "        if soup:\n",
    "            return soup, snap\n",
    "    return None, None\n",
    "\n",
    "def clean_text(el):\n",
    "    if not el:\n",
    "        return \"\"\n",
    "    for icon in el.select(\".i-heartxp\"):\n",
    "        icon.decompose()\n",
    "    return el.get_text(\" \", strip=True)\n",
    "\n",
    "def extract_phone(soup):\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if \"tel:\" in href:\n",
    "            phone = href.split(\"tel:\", 1)[-1]\n",
    "            phone = re.sub(r\"[^0-9+]\", \"\", phone)\n",
    "            if phone:\n",
    "                return phone\n",
    "\n",
    "    for a in soup.select(\".clinic-metabox a\"):\n",
    "        text = a.get_text(strip=True)\n",
    "        digits = re.sub(r\"[^0-9+]\", \"\", text)\n",
    "        if len(digits) >= 8:\n",
    "            return digits\n",
    "\n",
    "    return \"NA\"\n",
    "\n",
    "def extract_services(soup):\n",
    "    services = []\n",
    "    content = soup.select_one(\".entry-content\")\n",
    "    if content:\n",
    "        for ul in content.find_all(\"ul\"):\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                text = li.get_text(strip=True)\n",
    "                if text:\n",
    "                    services.append(text)\n",
    "\n",
    "    for card in soup.select(\".clinic-2020-services .featured-post-content h3 a\"):\n",
    "        text = card.get_text(strip=True)\n",
    "        if text:\n",
    "            services.append(text)\n",
    "\n",
    "    services = list(dict.fromkeys(services))  # dedupe, preserve order\n",
    "\n",
    "    if not services:\n",
    "        return \"NA\"\n",
    "\n",
    "    return \"; \".join(services)\n",
    "\n",
    "# Load existing CSV (for upsert)\n",
    "existing = {}\n",
    "\n",
    "if os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            key = (row[\"Name of Clinic\"].strip(), row[\"Address\"].strip())\n",
    "\n",
    "            # normalize empty fields\n",
    "            row[\"Email\"] = row[\"Email\"].strip() or \"NA\"\n",
    "            row[\"Phone\"] = row[\"Phone\"].strip() or \"NA\"\n",
    "            row[\"Services\"] = row[\"Services\"].strip() or \"NA\"\n",
    "\n",
    "            existing[key] = row\n",
    "\n",
    "print(f\"Loaded {len(existing)} existing clinics\")\n",
    "\n",
    "failed_regions = []\n",
    "failed_clinics = []\n",
    "\n",
    "# Discover regions\n",
    "main_soup, _ = get_soup_with_snapshots(\"/our-clinics/\", DEFAULT_SNAPSHOTS)\n",
    "\n",
    "if not main_soup:\n",
    "    raise SystemExit(\"Failed to load main clinics page\")\n",
    "\n",
    "region_slugs = sorted({\n",
    "    a[\"href\"].rstrip(\"/\").split(\"/\")[-1]\n",
    "    for a in main_soup.select('a[href*=\"/our-clinics/regions/\"]')\n",
    "})\n",
    "\n",
    "print(f\"Regions found: {len(region_slugs)}\")\n",
    "\n",
    "# Process regions & clinics (UPsert in memory)\n",
    "try:\n",
    "    for slug in region_slugs:\n",
    "        snapshots = REGION_SNAPSHOTS.get(slug, DEFAULT_SNAPSHOTS)\n",
    "\n",
    "        soup, used_snapshot = get_soup_with_snapshots(\n",
    "            f\"/our-clinics/regions/{slug}/\", snapshots\n",
    "        )\n",
    "\n",
    "        if not soup:\n",
    "            print(f\"REGION FAILED: {slug}\")\n",
    "            failed_regions.append(slug)\n",
    "            continue\n",
    "\n",
    "        region_url = build_url(used_snapshot, f\"/our-clinics/regions/{slug}/\")\n",
    "\n",
    "        clinic_links = [\n",
    "            urljoin(region_url, a[\"href\"])\n",
    "            for a in soup.select(\".div-table.regional-clinics .table-row a[href]\")\n",
    "        ]\n",
    "\n",
    "        clinic_links = sorted(set(clinic_links))\n",
    "        print(f\"REGION {slug}: {len(clinic_links)} clinics\")\n",
    "\n",
    "        for url in clinic_links:\n",
    "            print(f\"  Scraping {url}\")\n",
    "\n",
    "            path = \"/\" + url.split(BASE_SITE, 1)[-1].lstrip(\"/\")\n",
    "            soup, _ = get_soup_with_snapshots(path, [used_snapshot] + DEFAULT_SNAPSHOTS)\n",
    "\n",
    "            if not soup:\n",
    "                print(\"    FAILED\")\n",
    "                failed_clinics.append(url)\n",
    "                continue\n",
    "\n",
    "            name_el = soup.select_one(\"#clinic-metacard-2020 h1.entry-title\")\n",
    "            name = name_el.get_text(strip=True) if name_el else \"NA\"\n",
    "\n",
    "            address_el = soup.select_one(\".clinic-metabox .address\")\n",
    "            address = clean_text(address_el)\n",
    "\n",
    "            email = \"NA\"\n",
    "            for a in soup.select(\"#clinic-metacard-2020 a[href]\"):\n",
    "                if \"mailto:\" in a.get(\"href\", \"\"):\n",
    "                    email = a.get_text(strip=True)\n",
    "                    break\n",
    "\n",
    "            phone = extract_phone(soup)\n",
    "            services = extract_services(soup)\n",
    "            if not services or not services.strip():\n",
    "                services = \"NA\"\n",
    "\n",
    "\n",
    "            key = (name.strip(), address.strip())\n",
    "            action = \"UPDATED\" if key in existing else \"ADDED\"\n",
    "\n",
    "            existing[key] = {\n",
    "                \"Name of Clinic\": name,\n",
    "                \"Address\": address,\n",
    "                \"Email\": email,\n",
    "                \"Phone\": phone,\n",
    "                \"Services\": services,\n",
    "            }\n",
    "\n",
    "            print(f\"    {action}: {name}\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nInterrupted by user, saving progress...\")\n",
    "\n",
    "# Write final CSV (single authoritative write)\n",
    "with open(CSV_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "    for row in existing.values():\n",
    "        writer.writerow(row)\n",
    "\n",
    "if failed_regions:\n",
    "    with open(FAILED_REGIONS_FILE, \"w\") as f:\n",
    "        for r in sorted(set(failed_regions)):\n",
    "            f.write(r + \"\\n\")\n",
    "\n",
    "if failed_clinics:\n",
    "    with open(FAILED_CLINICS_FILE, \"w\") as f:\n",
    "        for c in sorted(set(failed_clinics)):\n",
    "            f.write(c + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(existing)} clinics\")\n",
    "print(f\"Failed regions: {len(set(failed_regions))}\")\n",
    "print(f\"Failed clinics: {len(set(failed_clinics))}\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d6d335",
   "metadata": {},
   "source": [
    "### Try failed clinics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e092f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying 0 failed clinics\n",
      "Recovered: 0\n",
      "Still failing: 0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "CSV_FILE = \"./myfootdr_clinics.csv\"\n",
    "FAILED_CLINICS_FILE = \"./failed_clinics.txt\"\n",
    "\n",
    "DEFAULT_SNAPSHOTS = [\n",
    "    \"20250708180027\",\n",
    "    \"20250517063937\",\n",
    "    \"20250516141742\",\n",
    "]\n",
    "\n",
    "BASE_SITE = \"https://www.myfootdr.com.au\"\n",
    "ARCHIVE_BASE = \"http://web.archive.org/web\"\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"Name of Clinic\",\n",
    "    \"Address\",\n",
    "    \"Email\",\n",
    "    \"Phone\",\n",
    "    \"Services\",\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept-Encoding\": \"identity\",\n",
    "    \"Connection\": \"close\",\n",
    "})\n",
    "\n",
    "def build_url(snapshot, path):\n",
    "    return f\"{ARCHIVE_BASE}/{snapshot}/{BASE_SITE}{path}\"\n",
    "\n",
    "def get_soup(url, retries=3):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            time.sleep(1.2)\n",
    "            r = session.get(url, timeout=40)\n",
    "            if r.status_code == 200 and len(r.text) > 500:\n",
    "                return BeautifulSoup(r.text, \"html.parser\")\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def get_soup_with_snapshots(path):\n",
    "    for snap in DEFAULT_SNAPSHOTS:\n",
    "        soup = get_soup(build_url(snap, path))\n",
    "        if soup:\n",
    "            return soup\n",
    "    return None\n",
    "\n",
    "def clean_text(el):\n",
    "    if not el:\n",
    "        return \"\"\n",
    "    for icon in el.select(\".i-heartxp\"):\n",
    "        icon.decompose()\n",
    "    return el.get_text(\" \", strip=True)\n",
    "\n",
    "def extract_phone(soup):\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if \"tel:\" in href:\n",
    "            phone = href.split(\"tel:\", 1)[-1]\n",
    "            phone = re.sub(r\"[^0-9+]\", \"\", phone)\n",
    "            if phone:\n",
    "                return phone\n",
    "    for a in soup.select(\".clinic-metabox a\"):\n",
    "        text = a.get_text(strip=True)\n",
    "        digits = re.sub(r\"[^0-9+]\", \"\", text)\n",
    "        if len(digits) >= 8:\n",
    "            return digits\n",
    "    return \"NA\"\n",
    "\n",
    "def extract_services(soup):\n",
    "    services = []\n",
    "    content = soup.select_one(\".entry-content\")\n",
    "    if content:\n",
    "        for ul in content.find_all(\"ul\"):\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                text = li.get_text(strip=True)\n",
    "                if text:\n",
    "                    services.append(text)\n",
    "\n",
    "    for card in soup.select(\".clinic-2020-services .featured-post-content h3 a\"):\n",
    "        text = card.get_text(strip=True)\n",
    "        if text:\n",
    "            services.append(text)\n",
    "\n",
    "    services = list(dict.fromkeys(services))  \n",
    "\n",
    "    if not services:\n",
    "        return \"NA\"\n",
    "\n",
    "    return \"; \".join(services)\n",
    "\n",
    "# Load existing CSV\n",
    "existing = {}\n",
    "\n",
    "with open(CSV_FILE, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        key = (row[\"Name of Clinic\"].strip(), row[\"Address\"].strip())\n",
    "        existing[key] = row\n",
    "\n",
    "# Load failed clinics\n",
    "with open(FAILED_CLINICS_FILE) as f:\n",
    "    failed_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "print(f\"Retrying {len(failed_urls)} failed clinics\")\n",
    "\n",
    "still_failed = []\n",
    "\n",
    "# Retry loop\n",
    "for idx, url in enumerate(failed_urls, start=1):\n",
    "    print(f\"[{idx}/{len(failed_urls)}] Retrying {url}\")\n",
    "\n",
    "    path = \"/\" + url.split(BASE_SITE, 1)[-1].lstrip(\"/\")\n",
    "    soup = get_soup_with_snapshots(path)\n",
    "\n",
    "    if not soup:\n",
    "        print(\"  still failed\")\n",
    "        still_failed.append(url)\n",
    "        continue\n",
    "\n",
    "    name_el = soup.select_one(\"#clinic-metacard-2020 h1.entry-title\")\n",
    "    name = name_el.get_text(strip=True) if name_el else \"NA\"\n",
    "\n",
    "    address_el = soup.select_one(\".clinic-metabox .address\")\n",
    "    address = clean_text(address_el)\n",
    "\n",
    "    email = \"NA\"\n",
    "    for a in soup.select(\"#clinic-metacard-2020 a[href]\"):\n",
    "        if \"mailto:\" in a.get(\"href\", \"\"):\n",
    "            email = a.get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    phone = extract_phone(soup)\n",
    "    services = extract_services(soup)\n",
    "\n",
    "    if not services.strip():\n",
    "        services = \"NA\"\n",
    "\n",
    "    key = (name.strip(), address.strip())\n",
    "\n",
    "    existing[key] = {\n",
    "        \"Name of Clinic\": name,\n",
    "        \"Address\": address,\n",
    "        \"Email\": email,\n",
    "        \"Phone\": phone,\n",
    "        \"Services\": services,\n",
    "    }\n",
    "\n",
    "    print(f\"  recovered: {name}\")\n",
    "\n",
    "# Rewrite CSV\n",
    "with open(CSV_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "    for row in existing.values():\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Rewrite failed list\n",
    "with open(FAILED_CLINICS_FILE, \"w\") as f:\n",
    "    for url in still_failed:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"Recovered: {len(failed_urls) - len(still_failed)}\")\n",
    "print(f\"Still failing: {len(still_failed)}\")\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9fb253",
   "metadata": {},
   "source": [
    "### Try failed regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63843e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrying 0 failed regions\n",
      "\n",
      "Summary\n",
      "Recovered regions: 0\n",
      "Still failing regions: 0\n",
      "New failed clinics added: 0\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "CSV_FILE = \"./myfootdr_clinics.csv\"\n",
    "FAILED_REGIONS_FILE = \"./failed_regions.txt\"\n",
    "FAILED_CLINICS_FILE = \"./failed_clinics.txt\"\n",
    "\n",
    "DEFAULT_SNAPSHOTS = [\n",
    "    \"20250708180027\",\n",
    "    \"20250517063937\",\n",
    "    \"20250516141742\",\n",
    "]\n",
    "\n",
    "BASE_SITE = \"https://www.myfootdr.com.au\"\n",
    "ARCHIVE_BASE = \"http://web.archive.org/web\"\n",
    "\n",
    "FIELDNAMES = [\n",
    "    \"Name of Clinic\",\n",
    "    \"Address\",\n",
    "    \"Email\",\n",
    "    \"Phone\",\n",
    "    \"Services\",\n",
    "]\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept-Encoding\": \"identity\",\n",
    "    \"Connection\": \"close\",\n",
    "})\n",
    "\n",
    "def build_url(snapshot, path):\n",
    "    return f\"{ARCHIVE_BASE}/{snapshot}/{BASE_SITE}{path}\"\n",
    "\n",
    "def get_soup(url, retries=3):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            time.sleep(1.2)\n",
    "            r = session.get(url, timeout=40)\n",
    "            if r.status_code == 200 and len(r.text) > 500:\n",
    "                return BeautifulSoup(r.text, \"html.parser\")\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def get_soup_with_snapshots(path):\n",
    "    for snap in DEFAULT_SNAPSHOTS:\n",
    "        soup = get_soup(build_url(snap, path))\n",
    "        if soup:\n",
    "            return soup, snap\n",
    "    return None, None\n",
    "\n",
    "def clean_text(el):\n",
    "    if not el:\n",
    "        return \"\"\n",
    "    for icon in el.select(\".i-heartxp\"):\n",
    "        icon.decompose()\n",
    "    return el.get_text(\" \", strip=True)\n",
    "\n",
    "def extract_phone(soup):\n",
    "    for a in soup.select(\"a[href]\"):\n",
    "        href = a.get(\"href\", \"\")\n",
    "        if \"tel:\" in href:\n",
    "            phone = href.split(\"tel:\", 1)[-1]\n",
    "            phone = re.sub(r\"[^0-9+]\", \"\", phone)\n",
    "            if phone:\n",
    "                return phone\n",
    "    for a in soup.select(\".clinic-metabox a\"):\n",
    "        digits = re.sub(r\"[^0-9+]\", \"\", a.get_text(strip=True))\n",
    "        if len(digits) >= 8:\n",
    "            return digits\n",
    "    return \"NA\"\n",
    "\n",
    "def extract_services(soup):\n",
    "    services = []\n",
    "\n",
    "    content = soup.select_one(\".entry-content\")\n",
    "    if content:\n",
    "        for ul in content.find_all(\"ul\"):\n",
    "            for li in ul.find_all(\"li\"):\n",
    "                text = li.get_text(strip=True)\n",
    "                if text:\n",
    "                    services.append(text)\n",
    "\n",
    "    for card in soup.select(\".clinic-2020-services .featured-post-content h3 a\"):\n",
    "        text = card.get_text(strip=True)\n",
    "        if text:\n",
    "            services.append(text)\n",
    "\n",
    "    services = list(dict.fromkeys(services))\n",
    "    return \"; \".join(services) if services else \"NA\"\n",
    "\n",
    "# Load CSV (UPSERT)\n",
    "existing = {}\n",
    "\n",
    "if os.path.exists(CSV_FILE):\n",
    "    with open(CSV_FILE, newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            key = (row[\"Name of Clinic\"].strip(), row[\"Address\"].strip())\n",
    "            existing[key] = row\n",
    "\n",
    "# Load failed regions\n",
    "with open(FAILED_REGIONS_FILE) as f:\n",
    "    failed_regions = [r.strip() for r in f if r.strip()]\n",
    "\n",
    "print(f\"Retrying {len(failed_regions)} failed regions\")\n",
    "\n",
    "recovered_regions = []\n",
    "still_failed_regions = []\n",
    "new_failed_clinics = []\n",
    "\n",
    "# Retry regions\n",
    "for slug in failed_regions:\n",
    "    print(f\"\\nREGION RETRY: {slug}\")\n",
    "\n",
    "    soup, snapshot = get_soup_with_snapshots(f\"/our-clinics/regions/{slug}/\")\n",
    "\n",
    "    if not soup:\n",
    "        print(\"  still failing\")\n",
    "        still_failed_regions.append(slug)\n",
    "        continue\n",
    "\n",
    "    recovered_regions.append(slug)\n",
    "\n",
    "    region_url = build_url(snapshot, f\"/our-clinics/regions/{slug}/\")\n",
    "\n",
    "    clinic_links = sorted({\n",
    "        urljoin(region_url, a[\"href\"])\n",
    "        for a in soup.select(\".div-table.regional-clinics .table-row a[href]\")\n",
    "    })\n",
    "\n",
    "    print(f\"  clinics found: {len(clinic_links)}\")\n",
    "\n",
    "    for url in clinic_links:\n",
    "        print(f\"    scraping {url}\")\n",
    "\n",
    "        path = \"/\" + url.split(BASE_SITE, 1)[-1].lstrip(\"/\")\n",
    "        soup, _ = get_soup_with_snapshots(path)\n",
    "\n",
    "        if not soup:\n",
    "            print(\"      FAILED\")\n",
    "            new_failed_clinics.append(url)\n",
    "            continue\n",
    "\n",
    "        name_el = soup.select_one(\"#clinic-metacard-2020 h1.entry-title\")\n",
    "        name = name_el.get_text(strip=True) if name_el else \"NA\"\n",
    "\n",
    "        address_el = soup.select_one(\".clinic-metabox .address\")\n",
    "        address = clean_text(address_el)\n",
    "\n",
    "        email = \"NA\"\n",
    "        for a in soup.select(\"#clinic-metacard-2020 a[href]\"):\n",
    "            if \"mailto:\" in a.get(\"href\", \"\"):\n",
    "                email = a.get_text(strip=True)\n",
    "                break\n",
    "\n",
    "        phone = extract_phone(soup)\n",
    "        services = extract_services(soup)\n",
    "\n",
    "        key = (name.strip(), address.strip())\n",
    "\n",
    "        existing[key] = {\n",
    "            \"Name of Clinic\": name,\n",
    "            \"Address\": address,\n",
    "            \"Email\": email,\n",
    "            \"Phone\": phone,\n",
    "            \"Services\": services,\n",
    "        }\n",
    "\n",
    "        print(f\"      OK: {name}\")\n",
    "\n",
    "# Rewrite CSV\n",
    "with open(CSV_FILE, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=FIELDNAMES)\n",
    "    writer.writeheader()\n",
    "    for row in existing.values():\n",
    "        writer.writerow(row)\n",
    "\n",
    "# Rewrite failed regions\n",
    "with open(FAILED_REGIONS_FILE, \"w\") as f:\n",
    "    for r in still_failed_regions:\n",
    "        f.write(r + \"\\n\")\n",
    "\n",
    "# Append newly failed clinics\n",
    "if new_failed_clinics:\n",
    "    with open(FAILED_CLINICS_FILE, \"a\") as f:\n",
    "        for url in sorted(set(new_failed_clinics)):\n",
    "            f.write(url + \"\\n\")\n",
    "\n",
    "print(\"\\nSummary\")\n",
    "print(f\"Recovered regions: {len(recovered_regions)}\")\n",
    "print(f\"Still failing regions: {len(still_failed_regions)}\")\n",
    "print(f\"New failed clinics added: {len(set(new_failed_clinics))}\")\n",
    "print(\"Done\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
